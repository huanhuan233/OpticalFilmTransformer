{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "UNK = 0\n",
    "PAD = 1\n",
    "\n",
    "from core.datasets.datasets import *\n",
    "from core.datasets.sim import *           # inc_tmm 在这里\n",
    "from core.models.transformer import *\n",
    "from core.trains.train import *\n",
    "\n",
    "# ================= 0) 光谱网格：380–750 nm，步长 5 nm =================\n",
    "def make_wavelengths():\n",
    "    return np.arange(380.0, 750.0 + 1e-9, 5.0)  # nm\n",
    "\n",
    "# ================= 1) 折射率表 =================\n",
    "lam_tab_tio2 = np.array([380.0,425.0,450.0,475.0,500.0,525.0,550.0,575.0,600.0,\n",
    "                         625.0,650.0,675.0,750.0,775.0,800.0,825.0,850.0,900.0,\n",
    "                         1000.0,1060.0])\n",
    "n_tab_tio2   = np.array([2.55,2.49,2.469,2.444,2.422,2.402,2.385,2.37,2.351,\n",
    "                         2.343,2.337,2.331,2.322,2.317,2.313,2.311,2.309,2.305,\n",
    "                         2.300,2.299])\n",
    "def n_tio2(lam_nm): return np.interp(lam_nm, lam_tab_tio2, n_tab_tio2)\n",
    "\n",
    "lam_tab_sio2 = np.array([300.0,350.0,400.0,450.0,500.0,550.0,600.0,650.0,700.0,900.0,1000.0])\n",
    "n_tab_sio2   = np.array([1.478 ,1.472 ,1.467 ,1.463 ,1.459 ,1.455 ,1.452 ,1.450 ,1.446 ,1.437 ,1.434])\n",
    "def n_sio2(lam_nm): return np.interp(lam_nm, lam_tab_sio2, n_tab_sio2)\n",
    "\n",
    "lam_tab_mgf2 = np.array([248.0, 550.0, 1550.0])\n",
    "n_tab_mgf2   = np.array([1.40 , 1.38 , 1.36  ])\n",
    "def n_mgf2(lam_nm): return np.interp(lam_nm, lam_tab_mgf2, n_tab_mgf2)\n",
    "\n",
    "glass_n_const = 1.5163\n",
    "def n_glass(lam_nm): return np.full_like(lam_nm, glass_n_const, dtype=float)\n",
    "\n",
    "# ================= 2) 网格与 nk_dict =================\n",
    "lam_nm = make_wavelengths()                # nm\n",
    "wavelengths = lam_nm / 1e3                 # µm\n",
    "nk_dict = {\n",
    "    'TiO2': n_tio2(lam_nm).astype(np.complex128),\n",
    "    'SiO2': n_sio2(lam_nm).astype(np.complex128),\n",
    "    'MgF2': n_mgf2(lam_nm).astype(np.complex128),\n",
    "    'Glass_Substrate': n_glass(lam_nm).astype(np.complex128),\n",
    "}\n",
    "\n",
    "# ================= 3) TMM 包装：返回 [R..., T...] =================\n",
    "def spectrum(materials, thickness, pol='s', theta=0, wavelengths=None,\n",
    "             nk_dict=None, substrate='Glass_Substrate', substrate_thick=500000):\n",
    "    assert len(materials) == len(thickness)\n",
    "    assert nk_dict is not None and wavelengths is not None\n",
    "    assert pol in ('s','p','u')\n",
    "\n",
    "    theta_rad = theta * (math.pi / 180.0)\n",
    "    wavess = (1e3 * np.asarray(wavelengths)).astype(int)\n",
    "\n",
    "    thickness_full = [np.inf] + list(thickness) + [substrate_thick, np.inf]\n",
    "    inc_list = ['i'] + ['c'] * len(materials) + ['i', 'i']\n",
    "\n",
    "    R, T = [], []\n",
    "    def _rt(pol_char, n_list, d_list, inc_list, theta_in, lambda_vac):\n",
    "        res = inc_tmm(pol_char, n_list, d_list, inc_list, theta_in, lambda_vac)\n",
    "        return res['R'], res['T']\n",
    "\n",
    "    for i, lambda_vac in enumerate(wavess):\n",
    "        n_list = [1.0] + [nk_dict[m][i] for m in materials] + [nk_dict[substrate][i], 1.0]\n",
    "        if pol == 'u':\n",
    "            rs, ts = _rt('s', n_list, thickness_full, inc_list, theta_rad, lambda_vac)\n",
    "            rp, tp = _rt('p', n_list, thickness_full, inc_list, theta_rad, lambda_vac)\n",
    "            r, t = 0.5*(rs+rp), 0.5*(ts+tp)\n",
    "        else:\n",
    "            r, t = _rt(pol, n_list, thickness_full, inc_list, theta_rad, lambda_vac)\n",
    "        R.append(float(r)); T.append(float(t))\n",
    "    return R + T\n",
    "\n",
    "# # ================= 4) Matplotlib 样式 =================\n",
    "# import matplotlib as mpl\n",
    "# mpl.rcParams['figure.dpi'] = 120\n",
    "# mpl.rcParams['figure.figsize'] = (6, 4)\n",
    "# mpl.rcParams.update({'font.size': 15, 'axes.titlesize': 15, 'axes.labelsize': 15, 'axes.titlepad': 10})\n",
    "\n",
    "# import matplotlib.font_manager as fm\n",
    "# try:\n",
    "#     fm.fontManager.addfont('/usr/share/fonts/truetype/msttcorefonts/arial.ttf')\n",
    "#     plt.rcParams['font.family'] = 'Arial'\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# ================= 5) 加载模型（只做推理） =================\n",
    "a = torch.load('/home/sysadmin/WorkSpace/PXY/optogpt/optogpt/saved_models/optogpt/test/model_inverse_R_T_S_R_B_LR_WU_L_H_D_F_[2, 0.1, 512, 0.0003, 1200, 6, 8, 512, 2048]_best.pt')\n",
    "args = a['configs']\n",
    "torch.manual_seed(args.seeds); np.random.seed(args.seeds)\n",
    "\n",
    "model = make_model_I(args.spec_dim, args.struc_dim, args.layers, args.d_model, args.d_ff, args.head_num, args.dropout).to(DEVICE)\n",
    "model.load_state_dict(a['model_state_dict'])\n",
    "criterion = LabelSmoothing(args.struc_dim, padding_idx=0, smoothing=args.smoothing)\n",
    "count_params(model)\n",
    "\n",
    "# 仅为拿词典与分批工具：不必真的加载/覆盖训练数据\n",
    "TRAIN_FILE = './dataset/Structure_train.pkl'\n",
    "TRAIN_SPEC_FILE = './dataset/Spectrum_train.pkl'\n",
    "DEV_FILE = './dataset/Structure_dev.pkl'\n",
    "DEV_SPEC_FILE = './dataset/Spectrum_dev.pkl'\n",
    "data = PrepareData(TRAIN_FILE, TRAIN_SPEC_FILE, args.ratios, DEV_FILE, DEV_SPEC_FILE, args.batch_size, 'R_T', 'Inverse')\n",
    "data.struc_word_dict, data.struc_index_dict = a['configs'].struc_word_dict, a['configs'].struc_index_dict\n",
    "\n",
    "# ================= 6) 工具函数：解析/评估 =================\n",
    "SPECIAL = {\"BOS\",\"EOS\",\"PAD\",\"UNK\", None, \"\"}\n",
    "# 清洗 token 序列：丢弃 BOS/PAD/UNK，遇到 EOS 截断\n",
    "def clean_tokens(tokens):\n",
    "    out = []\n",
    "    for tok in tokens:\n",
    "        if tok in SPECIAL:\n",
    "            if tok == \"EOS\": break\n",
    "            continue\n",
    "        out.append(tok)\n",
    "    print(\"clean_tokens →\", out)  # 打印，可注释\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将 token 序列解析为 (materials, thicknesses)，最多保留 max_layers 层\n",
    "def return_mat_thick(tokens, max_layers=20):\n",
    "    tokens = clean_tokens(tokens)[:max_layers]  # ≤20层软裁剪\n",
    "    mats, thks = [], []\n",
    "    for tok in tokens:\n",
    "        s = str(tok)\n",
    "        if \"_\" not in s:\n",
    "            if mats and s.replace(\".\",\"\",1).isdigit():\n",
    "                mats.append(mats[-1]); thks.append(float(s))\n",
    "            continue\n",
    "         \n",
    "        mat, thk = s.split(\"_\", 1)\n",
    "        num = \"\".join(ch for ch in thk if ch.isdigit() or ch==\".\")\n",
    "        if not num: continue\n",
    "        thks.append(float(num)); mats.append(mat)\n",
    "    return mats, thks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 辅助判断是否为一维向量，可选校验长度 \n",
    "def _is_vector(x, n=None):\n",
    "    try: arr = np.asarray(x).squeeze()\n",
    "    except Exception: return False\n",
    "    if arr.ndim != 1: return False\n",
    "    return (n is None) or (arr.size == n)\n",
    "# 从多种返回格式里“稳健”取出 (R, T)，必要时从拼接向量中拆分；失败则报错\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_RT(spec_obj, wavelengths_len: int):\n",
    "    if isinstance(spec_obj, (list, tuple)):\n",
    "        if len(spec_obj) >= 2 and all(_is_vector(v) for v in spec_obj[:2]):\n",
    "            return np.asarray(spec_obj[0]).squeeze(), np.asarray(spec_obj[1]).squeeze()\n",
    "    if isinstance(spec_obj, dict):\n",
    "        keys = {str(k).lower(): k for k in spec_obj.keys()}\n",
    "        if \"r\" in keys and \"t\" in keys:\n",
    "            return np.asarray(spec_obj[keys[\"r\"]]).squeeze(), np.asarray(spec_obj[keys[\"t\"]]).squeeze()\n",
    "        r_key = next((k for k in spec_obj if str(k).lower().startswith(\"r\")), None)\n",
    "        t_key = next((k for k in spec_obj if str(k).lower().startswith(\"t\")), None)\n",
    "        if r_key is not None and t_key is not None:\n",
    "            return np.asarray(spec_obj[r_key]).squeeze(), np.asarray(spec_obj[t_key]).squeeze()\n",
    "    for attr_r in (\"R\",\"r\"):\n",
    "        for attr_t in (\"T\",\"t\"):\n",
    "            if hasattr(spec_obj, attr_r) and hasattr(spec_obj, attr_t):\n",
    "                return np.asarray(getattr(spec_obj, attr_r)).squeeze(), np.asarray(getattr(spec_obj, attr_t)).squeeze()\n",
    "    arr = np.asarray(spec_obj).squeeze()\n",
    "    wl = wavelengths_len\n",
    "    if arr.ndim == 1 and arr.size == 2 * wl:\n",
    "        first, second = arr[:wl], arr[wl:]\n",
    "        if np.all((first>=0)&(first<=1)) and np.all((second>=0)&(second<=1)):\n",
    "            return first, second\n",
    "        return second, first\n",
    "    raise ValueError(\"extract_RT: 无法解析 (R,T)。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 将 R/T 两段同长向量拼接为单个特征向量 [R..., T...]\n",
    "def assemble_spec_vector(R, T):\n",
    "    R = np.asarray(R, dtype=np.float32).reshape(-1)\n",
    "    T = np.asarray(T, dtype=np.float32).reshape(-1)\n",
    "    assert R.shape == T.shape, f\"R/T 长度不一致: {R.shape} vs {T.shape}\"\n",
    "    return np.concatenate([R, T], axis=0)\n",
    "# 功能：加权 MAE（默认普通 MAE）；w 是逐元素权重向量\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_mae(y_true, y_pred, w=None):\n",
    "    a = np.asarray(y_true, dtype=np.float64)\n",
    "    b = np.asarray(y_pred, dtype=np.float64)\n",
    "    if w is None: return float(np.mean(np.abs(a-b)))\n",
    "    w = np.asarray(w, dtype=np.float64)\n",
    "    return float(np.sum(np.abs(a-b)*w) / (np.sum(w)+1e-12))\n",
    "# 将通道,波长,值[,权重]文本解析为与 wavelengths_nm 对齐的 R_target / T_target（支持百分数写法）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_user_targets(directives_text: str, wavelengths_nm, \n",
    "                       default_R: float = 0.0, default_T: float = 0.0,\n",
    "                       clip01: bool = True):\n",
    "    \"\"\"\n",
    "    将用户的锚点文本（每行: 通道, 波长(nm), 值[, 权重]）解析为与 wavelengths_nm 对齐的 (R_target, T_target) 数组。\n",
    "    规则：\n",
    "      - 同一通道给出 >=2 个点：对该通道做分段线性插值；区间外用端点值外延（hold）。\n",
    "      - 只给 1 个点：整段常数。\n",
    "      - 未给点：用默认值（default_R / default_T）。\n",
    "    说明：\n",
    "      - 文本里的第4列“权重”仅用于权重构造函数，这里忽略。\n",
    "      - 值默认为 0~1，小数或百分数都支持（>1 视为百分比并自动 /100）。\n",
    "    \"\"\"\n",
    "    wl = np.asarray(wavelengths_nm, dtype=float).reshape(-1)\n",
    "    N = wl.size\n",
    "\n",
    "    # 收集锚点\n",
    "    pts = {\"R\": [], \"T\": []}\n",
    "    lines = [ln.strip() for ln in str(directives_text).strip().splitlines() if ln.strip()]\n",
    "    for raw in lines:\n",
    "        parts = [p.strip() for p in raw.split(',')]\n",
    "        if len(parts) < 3:\n",
    "            raise ValueError(f\"格式错误: {raw}，应为 通道, 波长, 值[, 权重]\")\n",
    "        ch = parts[0].upper()\n",
    "        if ch not in (\"R\", \"T\"):\n",
    "            raise ValueError(f\"未知通道: {ch}（只支持 R/T）\")\n",
    "        lam = float(parts[1])\n",
    "        val = float(parts[2])\n",
    "        # 支持百分比写法：>1 视为百分数\n",
    "        if val > 1.0: \n",
    "            val = val / 100.0\n",
    "        pts[ch].append((lam, val))\n",
    "\n",
    "    def build_channel(target_points, default_val):\n",
    "        if len(target_points) == 0:\n",
    "            arr = np.full(N, float(default_val), dtype=np.float32)\n",
    "        elif len(target_points) == 1:\n",
    "            v = float(target_points[0][1])\n",
    "            arr = np.full(N, v, dtype=np.float32)\n",
    "        else:\n",
    "            # 对该通道做分段线性插值，并在边界外延\n",
    "            target_points = sorted(target_points, key=lambda x: x[0])\n",
    "            xs = np.array([p[0] for p in target_points], dtype=float)\n",
    "            ys = np.array([p[1] for p in target_points], dtype=float)\n",
    "            arr = np.interp(wl, xs, ys, left=ys[0], right=ys[-1]).astype(np.float32)\n",
    "        if clip01:\n",
    "            np.clip(arr, 0.0, 1.0, out=arr)\n",
    "        return arr\n",
    "\n",
    "    R = build_channel(pts[\"R\"], default_R)\n",
    "    T = build_channel(pts[\"T\"], default_T)\n",
    "    return R, T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_user_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 202\u001b[0m\n\u001b[1;32m    195\u001b[0m user_directives_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124mR,385,0.42,2\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124mT,550,0.80,0.8\u001b[39m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124mR,700,0.30,3\u001b[39m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# 1) 解析用户目标与权重（基于 lam_nm 网格）\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m R_target, T_target \u001b[38;5;241m=\u001b[39m \u001b[43mparse_user_targets\u001b[49m(user_directives_text, lam_nm)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;66;03m# 只在指定点计分（其余点=0）；如需带宽权重，改用 gaussian 版本\u001b[39;00m\n\u001b[1;32m    205\u001b[0m spec_weights \u001b[38;5;241m=\u001b[39m build_spec_weights_from_points(lam_nm, user_directives_text, base\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_user_targets' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 权重：默认只在指定点计分（其他点=0） ===\n",
    "# 仅在指定波长点处赋权（其余为 base，默认 0），拼接成 [wR..., wT...]\n",
    "def build_spec_weights_from_points(wavelengths_nm, directives_text, base=0.0):\n",
    "    wl = np.asarray(wavelengths_nm, dtype=float).reshape(-1)  # nm\n",
    "    N = wl.size\n",
    "    wR = np.full(N, base, dtype=np.float32)\n",
    "    wT = np.full(N, base, dtype=np.float32)\n",
    "    lines = [ln.strip() for ln in directives_text.strip().splitlines() if ln.strip()]\n",
    "    for raw in lines:\n",
    "        parts = [p.strip() for p in raw.split(',')]\n",
    "        if len(parts) < 4:\n",
    "            raise ValueError(f\"格式错误: {raw}，应为 通道, 波长, 值, 权重\")\n",
    "        ch, lam, _val, alpha = parts[0].upper(), float(parts[1]), float(parts[2]), float(parts[3])\n",
    "        idx = int(np.argmin(np.abs(wl - lam)))\n",
    "        if ch == \"R\": wR[idx] = float(alpha)\n",
    "        elif ch == \"T\": wT[idx] = float(alpha)\n",
    "        else: raise ValueError(f\"未知通道 {ch}\")\n",
    "    return np.concatenate([wR, wT], axis=0)\n",
    "\n",
    "# （可选）高斯带宽版：把单点权重扩成邻域，bring me shiiiiiiiiit \n",
    "# 在指定点附近按高斯展开权重（σ=sigma_nm），再拼接 [wR..., wT...]\n",
    "def build_spec_weights_from_points_gaussian(wavelengths_nm, directives_text, sigma_nm=15.0, base=0.0):\n",
    "    wl = np.asarray(wavelengths_nm, dtype=float).reshape(-1)\n",
    "    N = wl.size\n",
    "    wR = np.full(N, base, dtype=np.float32)\n",
    "    wT = np.full(N, base, dtype=np.float32)\n",
    "    lines = [ln.strip() for ln in directives_text.strip().splitlines() if ln.strip()]\n",
    "    for raw in lines:\n",
    "        parts = [p.strip() for p in raw.split(',')]\n",
    "        if len(parts) < 4:\n",
    "            raise ValueError(f\"格式错误: {raw}\")\n",
    "        ch, lam, _val, alpha = parts[0].upper(), float(parts[1]), float(parts[2]), float(parts[3])\n",
    "        g = np.exp(-0.5 * ((wl - lam)/sigma_nm)**2)\n",
    "        g = g / (g.max() + 1e-12) * float(alpha)\n",
    "        if ch == \"R\": wR = np.maximum(wR, g.astype(np.float32))\n",
    "        elif ch == \"T\": wT = np.maximum(wT, g.astype(np.float32))\n",
    "        else: raise ValueError(f\"未知通道 {ch}\")\n",
    "    return np.concatenate([wR, wT], axis=0)\n",
    "\n",
    "# ================= 7) 解码辅助：约束与抽样 =================\n",
    "# 按已输出层数对下一个 token 的概率分布做规则屏蔽（禁止 BOS；超层数仅允许 EOS）\n",
    "def mask_invalid_next(probs, word_dict, step_layers, max_layers=20):\n",
    "    \"\"\"\n",
    "    probs: Tensor [1, vocab] 或 [vocab] 的概率分布（不是log）\n",
    "    step_layers: 当前已输出的“结构”层数（不含BOS/EOS）\n",
    "    规则：\n",
    "      - 若 step_layers >= max_layers：只保留 EOS\n",
    "      - 否则：禁止 BOS，其余结构token+EOS 均可\n",
    "    \"\"\"\n",
    "    if probs.dim() == 2: probs = probs[0]\n",
    "    probs = probs.clone()\n",
    "    eos_id = word_dict['EOS']\n",
    "    bos_id = word_dict['BOS']\n",
    "\n",
    "    if step_layers >= max_layers:\n",
    "        keep = torch.zeros_like(probs); keep[eos_id] = 1.0\n",
    "        probs = probs * keep\n",
    "    else:\n",
    "        probs[bos_id] = 0.0\n",
    "\n",
    "    s = probs.sum()\n",
    "    if s.item() <= 0:\n",
    "        probs[:] = 0.0\n",
    "        probs[eos_id] = 1.0\n",
    "    else:\n",
    "        probs = probs / s\n",
    "    return probs.unsqueeze(0)\n",
    "#对 logits 施加 Top-K + Top-P（核采样）双重截断，保留原 shape 的“masked logits” ？？？？\n",
    "def apply_top_k_top_p(logits, top_k=10, top_p=0.8):\n",
    "    \"\"\"\n",
    "    输入 logits（对数几率），返回截断后的 logits（保留原 shape）\n",
    "    \"\"\"\n",
    "    top_k = max(1, int(top_k))\n",
    "    sorted_logits, sorted_idx = torch.sort(logits, dim=-1, descending=True)\n",
    "    # top-k\n",
    "    if top_k < logits.size(-1):\n",
    "        thresh = sorted_logits[..., top_k-1:top_k]\n",
    "        remove = sorted_logits < thresh\n",
    "        sorted_logits = sorted_logits.masked_fill(remove, float('-inf'))\n",
    "    # top-p\n",
    "    probs_sorted = torch.softmax(sorted_logits, dim=-1)\n",
    "    cum = probs_sorted.cumsum(dim=-1)\n",
    "    remove = cum > top_p\n",
    "    remove[..., 0] = False\n",
    "    sorted_logits = sorted_logits.masked_fill(remove, float('-inf'))\n",
    "\n",
    "    out = torch.full_like(logits, float('-inf'))\n",
    "    out.scatter_(1, sorted_idx, sorted_logits)\n",
    "    return out\n",
    "\n",
    "# ================= 8) 解码器：强制 ≤20 层 + 必有 EOS =================\n",
    "# 带层数 ≤ max_layers + 提前偏置 EOS的贪心解码，返回（原始token、清洗token、每步概率）\n",
    "# 可以设定最大层数\n",
    "def greedy_decode_w(model, struc_word_dict, R_target, T_target, max_len,\n",
    "                    start_symbol=\"BOS\", spec_weights=None, device=None,\n",
    "                    eos_bias_after=15, eos_bias_logit=1.5, max_layers=20):\n",
    "    DEVICE = device or next(model.parameters()).device\n",
    "    id2tok = {v: k for k, v in struc_word_dict.items()}\n",
    "    BOS_id = struc_word_dict.get(start_symbol, 2)\n",
    "    EOS_id = struc_word_dict['EOS']\n",
    "    PAD_id = 0\n",
    "\n",
    "    # 不把权重乘进模型输入！\n",
    "    spec_vec = assemble_spec_vector(R_target, T_target)\n",
    "    src = torch.tensor([spec_vec], dtype=torch.float32, device=DEVICE)\n",
    " \n",
    "    ys = torch.tensor([[BOS_id]], dtype=torch.long, device=DEVICE)\n",
    "    step_layers = 0\n",
    "    probs_hist = []\n",
    "\n",
    "    for _ in range(max_len - ys.size(1)):   \n",
    "        tgt_mask = (ys != PAD_id).unsqueeze(-2) & (subsequent_mask(ys.size(-1)).to(ys.device))\n",
    "        out = model(src, ys, src_mask=None, tgt_mask=tgt_mask)\n",
    "        dec_last = out[:, -1, :]                      # [1, d_model]\n",
    "        logp = model.generator(dec_last)              # [1, vocab] (log_softmax)\n",
    "        if step_layers >= eos_bias_after:\n",
    "            logp[:, EOS_id] = logp[:, EOS_id] + eos_bias_logit\n",
    "\n",
    "        probs = torch.exp(logp)\n",
    "        probs = mask_invalid_next(probs, struc_word_dict, step_layers, max_layers=max_layers)\n",
    "\n",
    "        next_id = int(torch.argmax(probs, dim=-1).item())\n",
    "        probs_hist.append(float(probs[0, next_id].item()))\n",
    "        ys = torch.cat([ys, torch.tensor([[next_id]], dtype=torch.long, device=DEVICE)], dim=1)\n",
    "\n",
    "        if next_id == EOS_id: break\n",
    "        step_layers += 1\n",
    "\n",
    "    if step_layers >= max_layers and ys[0, -1].item() != EOS_id:\n",
    "        ys = torch.cat([ys, torch.tensor([[EOS_id]], dtype=torch.long, device=DEVICE)], dim=1)\n",
    "\n",
    "    raw_tokens = [id2tok.get(tid, None) for tid in ys[0].tolist()]\n",
    "    cleaned_tokens = []\n",
    "    for tok in raw_tokens:\n",
    "        if tok in {\"BOS\",\"PAD\",\"UNK\", None, \"\"}: continue\n",
    "        if tok == \"EOS\": break\n",
    "        cleaned_tokens.append(tok)\n",
    "\n",
    "    return raw_tokens, cleaned_tokens, probs_hist\n",
    "# Top-k + Top-p 采样多样化解码（可选指定首层材料），同样限制层数与 EOS 偏置；返回（tokens, 概率轨迹）\n",
    "def top_k_n_w(k, top_p, model, struc_word_dict, R_target, T_target, max_len,\n",
    "              start_symbol=\"BOS\", spec_weights=None, device=None,\n",
    "              temperature=1.0, start_mat=None, eos_bias_after=15,\n",
    "              eos_bias_logit=1.5, max_layers=20):\n",
    "    DEVICE = device or next(model.parameters()).device\n",
    "    id2tok = {v: k for k, v in struc_word_dict.items()}\n",
    "    BOS_id  = struc_word_dict.get(start_symbol, 2)\n",
    "    EOS_id  = struc_word_dict['EOS']\n",
    "    PAD_id  = 0\n",
    "\n",
    "    # 不要权重乘进模型输入啊啊啊啊！\n",
    "    spec_vec = assemble_spec_vector(R_target, T_target)\n",
    "    src = torch.tensor([spec_vec], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    if start_mat:\n",
    "        ys = torch.tensor([[BOS_id, struc_word_dict[start_mat]]], dtype=torch.long, device=DEVICE)\n",
    "        step_layers = 1\n",
    "    else:\n",
    "        ys = torch.tensor([[BOS_id]], dtype=torch.long, device=DEVICE)\n",
    "        step_layers = 0\n",
    "\n",
    "    probs_hist = []\n",
    "    for _ in range(max_len - ys.size(1)):\n",
    "        tgt_mask = (ys != PAD_id).unsqueeze(-2) & (subsequent_mask(ys.size(-1)).to(ys.device))\n",
    "        out = model(src, ys, src_mask=None, tgt_mask=tgt_mask)\n",
    "        dec_last = out[:, -1, :]\n",
    "        logp = model.generator(dec_last) / max(1e-6, float(temperature))\n",
    "        if step_layers >= eos_bias_after:\n",
    "            logp[:, EOS_id] = logp[:, EOS_id] + eos_bias_logit\n",
    "\n",
    "        logits_kept = apply_top_k_top_p(logp, top_k=k, top_p=top_p)\n",
    "        probs = torch.softmax(logits_kept, dim=-1)\n",
    "        probs = mask_invalid_next(probs, struc_word_dict, step_layers, max_layers=max_layers)\n",
    "\n",
    "        next_id = int(torch.multinomial(probs, num_samples=1).item())\n",
    "        probs_hist.append(float(probs[0, next_id].item()))\n",
    "        ys = torch.cat([ys, torch.tensor([[next_id]], dtype=torch.long, device=DEVICE)], dim=1)\n",
    "\n",
    "        if next_id == EOS_id: break\n",
    "        step_layers += 1\n",
    "\n",
    "    if step_layers >= max_layers and ys[0, -1].item() != EOS_id:\n",
    "        ys = torch.cat([ys, torch.tensor([[EOS_id]], dtype=torch.long, device=DEVICE)], dim=1)\n",
    "\n",
    "    tokens = []\n",
    "    for tid in ys[0].tolist():\n",
    "        tok = id2tok.get(tid, None)\n",
    "        if tok in (None, 'BOS', 'PAD', 'UNK'): continue\n",
    "        if tok == 'EOS': break\n",
    "        tokens.append(tok)\n",
    "    return tokens, probs_hist\n",
    "\n",
    "# ================= 9) 用户点 → 目标/权重 → 逆设计与评估 =================\n",
    "#默认是1 ， 用户可以限制最大层数\n",
    "user_directives_text = \"\"\"\n",
    "R,385,0.42,2\n",
    "T,550,0.80,0.8\n",
    "R,700,0.30,3\n",
    "\"\"\".strip()\n",
    "\n",
    "# 1) 解析用户目标与权重（基于 lam_nm 网格）\n",
    "R_target, T_target = parse_user_targets(user_directives_text, lam_nm)\n",
    "\n",
    "# 只在指定点计分（其余点=0）；如需带宽权重，改用 gaussian 版本\n",
    "spec_weights = build_spec_weights_from_points(lam_nm, user_directives_text, base=0.0)\n",
    "# spec_weights = build_spec_weights_from_points_gaussian(lam_nm, user_directives_text, sigma_nm=15.0, base=0.0)\n",
    "\n",
    "N = len(lam_nm)\n",
    "assert len(R_target) == N and len(T_target) == N, f\"目标长度应为 {N}\"\n",
    "\n",
    "spec_target_vec = assemble_spec_vector(R_target, T_target)\n",
    "\n",
    "# 2) 解码 + 评估\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # ---- 贪心 ----\n",
    "    raw_tokens, cleaned_tokens, probs_g = greedy_decode_w(\n",
    "        model, data.struc_word_dict, R_target, T_target,\n",
    "        max_len=getattr(args, \"max_len\", 22),\n",
    "        start_symbol=\"BOS\",\n",
    "        spec_weights=None,     # ⬅️ 不传或传 None，避免乘到输入\n",
    "        device=DEVICE\n",
    "    )\n",
    "    print(\"[Greedy] raw tokens:  \", raw_tokens)\n",
    "    print(\"[Greedy] clean tokens:\", cleaned_tokens)\n",
    " \n",
    "    # ---------- 评估一个结构 ----------\n",
    "    def eval_structure(tokens_list):\n",
    "        if not tokens_list:\n",
    "            zR, zT = np.zeros_like(R_target), np.zeros_like(T_target)\n",
    "            return float(\"inf\"), float(\"inf\"), (zR, zT)\n",
    "        try:\n",
    "            mats, thks = return_mat_thick(tokens_list)\n",
    "            spec_obj = spectrum(\n",
    "                mats, thks, wavelengths=wavelengths, nk_dict=nk_dict,\n",
    "                substrate='Glass_Substrate', substrate_thick=500000\n",
    "            )\n",
    "            R_sim, T_sim = extract_RT(spec_obj, wavelengths_len=len(wavelengths))\n",
    "            vec_sim = assemble_spec_vector(R_sim, T_sim)\n",
    "            mae_plain  = float(np.mean(np.abs(vec_sim - spec_target_vec)))\n",
    "            mae_weight = weighted_mae(spec_target_vec, vec_sim, w=spec_weights)\n",
    "            return mae_plain, mae_weight, (R_sim, T_sim)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] eval_structure failed:\", e)\n",
    "            zR, zT = np.zeros_like(R_target), np.zeros_like(T_target)\n",
    "            return float(\"inf\"), float(\"inf\"), (zR, zT)\n",
    "\n",
    "    mae_plain, mae_weight, (R_g, T_g) = eval_structure(cleaned_tokens)\n",
    "    print(\"[Greedy] MAE=%.6f | wMAE=%.6f\" % (mae_plain, mae_weight), cleaned_tokens)\n",
    "\n",
    "    # 记录最佳（先用贪心结果初始化，**以 wMAE 为准**）\n",
    "    best_score  = mae_weight\n",
    "    best_struct = cleaned_tokens\n",
    "    best_spec   = (R_g, T_g)\n",
    "    best_tag    = \"Greedy\"\n",
    "\n",
    "    # ---- 采样若干解：用 wMAE 选最优 ----\n",
    "    print(\"Top-kp sampling:\")\n",
    "    SAMPLES = 20\n",
    "    for kk in range(SAMPLES):\n",
    "        struc_k, probs_k = top_k_n_w(\n",
    "            k=10, top_p=0.8,\n",
    "            model=model, struc_word_dict=data.struc_word_dict,\n",
    "            R_target=R_target, T_target=T_target,\n",
    "            max_len=getattr(args, \"max_len\", 22),\n",
    "            start_symbol=\"BOS\",\n",
    "            spec_weights=None,   # ⬅️ 不乘权重\n",
    "            device=DEVICE\n",
    "        )\n",
    "\n",
    "        mae_p, mae_w, (R_k, T_k) = eval_structure(struc_k)\n",
    "        if not np.isfinite(mae_w):\n",
    "            print(f\"  Structure {kk:02d}: wMAE=  inf\", struc_k)\n",
    "            continue\n",
    "\n",
    "        print(f\"  Structure {kk:02d}: wMAE= {mae_w:.6f}\", struc_k)\n",
    "\n",
    "        if mae_w < best_score:\n",
    "            best_score  = mae_w\n",
    "            best_struct = struc_k\n",
    "            best_spec   = (R_k, T_k)\n",
    "            best_tag    = f\"Sample#{kk:02d}\"\n",
    "\n",
    "print(\"\\n=== Best by weighted MAE ===\")\n",
    "print(\"source:\", best_tag)\n",
    "print(\"wMAE:\", best_score)\n",
    "print(\"structure:\", best_struct)\n",
    "\n",
    "# 误差函数验证与TFSCALE的一致\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[saved] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msavepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# ====== 从 checkpoint 里取出曲线并绘图 ======\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m loss_all \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_all\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# 兼容你之前的结构：a['loss_all']['train_loss'] / ['dev_loss']\u001b[39;00m\n\u001b[1;32m     90\u001b[0m train_curve \u001b[38;5;241m=\u001b[39m loss_all\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# ================= 绘图：训练/验证曲线（自动兼容张量/标量/列表） =================\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Sequence, Any\n",
    "\n",
    "def _to_1d_list(x: Any):\n",
    "    \"\"\"\n",
    "    把 list/tuple/np.ndarray/torch.Tensor → 纯 Python list[float]\n",
    "    自动 .cpu().item()，自动展平。\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        if x.ndim == 0:\n",
    "            return [float(x.detach().cpu().item())]\n",
    "        return [float(t.detach().cpu().item()) for t in x.reshape(-1)]\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return [float(v) for v in x.reshape(-1)]\n",
    "    if isinstance(x, (list, tuple)):\n",
    "        out = []\n",
    "        for v in x:\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                out.append(float(v.detach().cpu().item()))\n",
    "            elif isinstance(v, (np.ndarray, list, tuple)):\n",
    "                out.extend(_to_1d_list(v))\n",
    "            else:\n",
    "                out.append(float(v))\n",
    "        return out\n",
    "    # 单个标量\n",
    "    try:\n",
    "        return [float(x)]\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _moving_average(y: Sequence[float], k: int):\n",
    "    if y is None or len(y) == 0 or k <= 1:\n",
    "        return y\n",
    "    k = min(k, len(y))\n",
    "    pad = k // 2\n",
    "    y_pad = np.pad(np.asarray(y, dtype=float), (pad, pad), mode='edge')\n",
    "    kernel = np.ones(k, dtype=float) / k\n",
    "    return np.convolve(y_pad, kernel, mode='valid').tolist()\n",
    "\n",
    "def plot_train_curves(loss_train, loss_eval, *,\n",
    "                      title=\"Training Curve\",\n",
    "                      ylabel=\"Loss\",\n",
    "                      smooth=0,         # 平滑窗口（奇数，0/1 表示不平滑）\n",
    "                      logy=True,        # y 轴取对数\n",
    "                      savepath=\"./training_curve.png\"):\n",
    "    lt = _to_1d_list(loss_train) or []\n",
    "    lv = _to_1d_list(loss_eval)  or []\n",
    "    # 截齐长度（有些日志 eval 少于 train）\n",
    "    n = min(len(lt), len(lv)) if lv else len(lt)\n",
    "    x = list(range(1, n + 1))\n",
    "    lt = lt[:n]\n",
    "    if lv: lv = lv[:n]\n",
    "\n",
    "    lt_plot = _moving_average(lt, smooth) if smooth and smooth > 1 else lt\n",
    "    lv_plot = _moving_average(lv, smooth) if lv and smooth and smooth > 1 else lv\n",
    "\n",
    "    plt.figure(figsize=(7.5, 4.5), dpi=140)\n",
    "    plt.plot(x, lt_plot, label=\"Training\", linewidth=2)\n",
    "    if lv_plot:\n",
    "        plt.plot(x, lv_plot, label=\"Validation\", linewidth=2)\n",
    "\n",
    "        # 标注最佳 epoch（取验证集最小值）\n",
    "        best_idx = int(np.argmin(lv))  # 原始未平滑上的最佳\n",
    "        best_x = best_idx + 1\n",
    "        best_y = lv[best_idx]\n",
    "        plt.axvline(best_x, color='gray', linestyle='--', linewidth=1)\n",
    "        plt.scatter([best_x], [best_y], s=35, zorder=5)\n",
    "        plt.text(best_x, best_y, f\"  best@{best_x}: {best_y:.4f}\", va='bottom', fontsize=10)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(ylabel)\n",
    "    if logy:\n",
    "        plt.yscale(\"log\")\n",
    "    plt.grid(True, alpha=0.25)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(savepath)\n",
    "    plt.show()\n",
    "    print(f\"[saved] {savepath}\")\n",
    "\n",
    "# ====== 从 checkpoint 里取出曲线并绘图 ======\n",
    "loss_all = a.get(\"loss_all\", {})\n",
    "# 兼容你之前的结构：a['loss_all']['train_loss'] / ['dev_loss']\n",
    "train_curve = loss_all.get(\"train_loss\", None)\n",
    "dev_curve   = loss_all.get(\"dev_loss\",   None)\n",
    "\n",
    "print(\"Final dev loss:\", (_to_1d_list(dev_curve) or [None])[-1])\n",
    "\n",
    "# 主图：KL loss 曲线（对数坐标，平滑窗口=3 可改 0/1 表示不平滑）\n",
    "plot_train_curves(train_curve, dev_curve,\n",
    "                  title=\"Training Curve (KL Loss)\",\n",
    "                  ylabel=\"KL Loss\",\n",
    "                  smooth=3,\n",
    "                  logy=True,\n",
    "                  savepath=\"./training_curve_kl.png\")\n",
    "\n",
    "# 若 ckpt 里还有其它可选曲线（可选：学习率、准确率等），也尝试绘图\n",
    "# 假设：loss_all 可能包含 'lrs' / 'train_acc' / 'dev_acc'\n",
    "lrs = loss_all.get(\"lrs\", None)\n",
    "if lrs is not None:\n",
    "    plot_train_curves(lrs, None,\n",
    "                      title=\"Learning Rate (per epoch)\",\n",
    "                      ylabel=\"LR\",\n",
    "                      smooth=1,\n",
    "                      logy=False,\n",
    "                      savepath=\"./training_curve_lr.png\")\n",
    "\n",
    "train_acc = loss_all.get(\"train_acc\", None)\n",
    "dev_acc   = loss_all.get(\"dev_acc\", None)\n",
    "if train_acc is not None or dev_acc is not None:\n",
    "    plot_train_curves(train_acc, dev_acc,\n",
    "                      title=\"Accuracy Curve\",\n",
    "                      ylabel=\"Accuracy\",\n",
    "                      smooth=3,\n",
    "                      logy=False,\n",
    "                      savepath=\"./training_curve_acc.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "nteract": {
   "version": "0.25.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "557px",
    "left": "447px",
    "right": "20px",
    "top": "255px",
    "width": "683px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
